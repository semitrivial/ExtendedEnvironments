\documentclass{article}
%\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}

\newtheorem{mytheorem}{Theorem}
\newtheorem{definition}[mytheorem]{Definition}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}
\def\ROA{(\mathcal R\mathcal O\mathcal A)^*}
\def\ROARO{(\mathcal R\mathcal O\mathcal A)^*\mathcal R\mathcal O}

\begin{document}

\title{Extending environments to incentivize self-reflection
in reinforcement learning}
%\titlerunning{Extending environments to incentivize self-reflection}

\author{Samuel Allen Alexander \& Michael Castaneda\\
\& Kevin Compher \& Oscar Martinez}


\maketitle

\begin{abstract}
    We consider an extended notion
    of reinforcement learning environment, in which the environment is able
    to simulate the agent. We give
    examples of some such extended environments which seem to incentivize
    various different types of self-reflection or self-awareness.
    These environments might shed light on how self-awareness is incentivized
    in evolution (where an organism's offspring approximate a kind of simulation
    of that organism). We
    hope these environments will guide the development of self-aware reinforcement
    learning agents, and will help measure the degree to which existing
    reinforcement learning agents are or are not self-aware. To that end,
    we have released an open-source library of such environments.
    We also speculate
    about subjective conscious experiences which
    might be incentivized in self-aware reinforcement learning agents
    placed within these extended environments.
\end{abstract}

\section{Introduction}

In order to motivate the environments in this paper (and in the accompanying
open-source library \cite{library} which we are publishing alongside the
paper),
we begin with three thought experiments.
\begin{itemize}
    \item (Oracular Obstacle Courses)
        An ordinary obstacle course might depend on what you \emph{do}:
        step on a button and spikes appear, for example.
        But imagine an obstacle course which depends on what you
        \emph{would hypothetically do}:
        you walk into a room with no button, but spikes appear if you
        \emph{would} hypothetically step on the button if there were one.
    \item (Treasures for the Worthy\footnote{This thought experiment bears some
        similarity to Newcomb's Paradox \cite{nozick1969newcomb}.})
        You wander through rooms, each containing a treasure chest.
        Most of the rooms also contain a guard. In a room with no guard, you
        may optionally choose to take the treasure, which acts as a free reward.
        In a guarded room, you may optionally try to take the treasure.
        If you do so, the guard will determine whether or not you would hypothetically
        take the treasure if the room were unguarded. If you would take the treasure
        if the room were unguarded, then the guard blocks the treasure and zaps you.
        But if you would leave the treasure alone if the room were unguarded, then
        the guard lets you take the treasure.
    \item (The Clever Employer)
        Your workplace demands that you act as if you're paid \$1000 per hour.
        But instead of paying you \$1000 per hour, here's what they do instead.
        Every hour, if you acted exactly as if you're paid \$1000 per hour,
        they pay you \$100. But every hour, if you did not act exactly as if you're
        paid \$1000 per hour, then they pay you nothing, and charge you a \$100
        penalty instead.
\end{itemize}

These thought experiments would be hard to perform on a human subject, because they
require knowing what the human subject would hypothetically do in various
counterfactual circumstances. But if the experiment administrator could simulate
a perfect copy of you, then the experiments would be doable. In the first two
experiments, would you eventually figure out the pattern? In the last experiment,
would you eventually begin to believe you were really being paid \$1000 per hour?

This is a paper about reinforcement learning (RL).
In RL, agents interact with environments, where they take actions and
receive rewards and observations in response to those actions.
For sake of simplicity, we restrict our attention
to deterministic environments and deterministic agents, but the basic idea
would easily adapt
to non-deterministic RL.

Although the details differ between authors, essentially,
an RL environment is a Turing machine (i.e., a computer program)
$e$ which outputs a reward-observation pair
\[
  \langle r,o\rangle=e(r_0,o_0,a_0,\ldots,r_n,o_n,a_n)
\]
in response to a
reward-observation-action sequence $(r_0,o_0,a_0,\ldots,r_n,o_n,a_n)$.
An RL agent is a Turing machine $T$ which outputs an action
\[
  a=T(r_0,o_0,a_0,\ldots,r_n,o_n)
\]
in response to a
reward-observation-action-$\cdots$-reward-observation sequence.
An RL agent can interact with an RL environment in an obvious way.
However, there is another type of environment in which
RL agents can interact just as well. We define an \emph{extended
environment} to be a Turing machine $e$ which outputs a reward-observation pair
\[
    \langle r,o\rangle=e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n \rangle)
\]
in response
to a reward-observation-action sequence
along with an RL agent $T$.
Intuitively, this should be thought of as follows: when the agent enters the environment,
the environment is made aware of the agent's source-code, and can use that
source-code to simulate the agent when computing rewards and observations.

For example, the three thought experiments in the beginning of this Introduction could
be cast as extended environments in which the environment somehow knows the agent's
source-code and uses that to determine what the agent would do in various
hypothetical situations.
Clearly such
extended environments are not the sort of environments RL agents are traditionally
intended to interact with\footnote{Such environments might, however,
accidentally arise if both environment
and agent are implemented on the same machine and the environment is managed by an AI
sophisticated enough to exploit unintended informational side channels, as in
\cite{yampolskiy2012leakproofing}.}. However, such environments could be
useful on the path to Artificial
General Intelligence (AGI) because they seem to incentivize self-awareness:
in order to figure out the environment, the agent is incentivized to look
within\footnote{To quote Goertzel: ``This lets us conceive an intelligent system
as a dynamical system that recognizes patterns in its environment \emph{and itself},
as part of its quest to achieve complex goals'' \cite{goertzel2006patterns}
(emph.\ in the original).}
and ask, ``How would I act in such-and-such alternative scenarios?''
Furthermore, if living organisms' agency-functions are inherited (perfectly or with
minor mutations) from their ancestors, then in some sense, related organisms are
approximately simulations of each other.
This suggests the possibility that such extended environments could play a role
in the evolution of self-awareness.

One might try to imitate an extended environment with a traditional environment by
backtracking---rewinding the environment itself to a prior state after seeing how the
agent performs along one path, and then sending the agent along a second path.
But the agent itself would retain memory of the first path, and the agent's decisions
along the second path might be altered by said memories. Thus the result would not be
the same as immediately sending the agent along the second path while secretly simulating
the agent to determine what it would do if sent along the first path.

We will give examples of
extended environments designed to incentivize RL agents to
recursively engage in self-awareness in various ways.
It would be interesting to measure the performance of industry-standard
traditional RL agents in these example environments, given that these agents
were not designed with such self-awareness in mind.
We hope these examples will facilitate
new self-aware RL techniques, hopefully as a step toward AGI.

Alongside this paper, we are also releasing an open-source library
\cite{library} of extended environments
implemented in python. The library also includes some machinery for running an
agent against a battery of extended environments in order to empirically
probe the agent's self-awareness (or lack thereof).

\section{Preliminaries}

Throughout the paper, $\frown$ denotes concatenation.

\begin{definition}
\label{historiesdefn}
    \begin{enumerate}
        \item
        By a \emph{reward}, we mean a rational number.
        By an \emph{observation}, we mean a natural number.
        By an \emph{action}, we mean a natural number.
        \item
        By $\ROA$
        we mean the set of all finite sequences which begin with
        a reward, end with an action, and follow the pattern
        ``reward, observation, action, ...''. We also include
        the empty sequence $\langle\rangle$ in this set.
        \item
        By $\ROARO$ we mean the set of all sequences of the form
        $s\frown r\frown o$ where $s\in \ROA$, $r$ is a reward, and
        $o$ is an observation.
    \end{enumerate}
\end{definition}

In Definition \ref{historiesdefn} part 3, the intuition is that in response to
history $t$, an environment rewards an agent with reward $r$ and
shows the agent an observation $o$.

\begin{mylemma}
\label{roaplaydecompositionlemma}
    If $s\in \ROA$, then either $s=\langle\rangle$, or else
    $s=t\frown a$ for some $t\in \ROARO$ and action $a\in\mathbb N$.
\end{mylemma}

\begin{proof}
    Trivial.
\end{proof}

In Lemma \ref{roaplaydecompositionlemma}, when
$s=t\frown a$, the intuition is that an agent, having been prompted to act by
history $t$, responds by taking action $a$.

\begin{definition}
\label{agentandenvironment}
(Agents and environments)
    \begin{enumerate}
    \item An \emph{agent} is a Turing machine $T$ such that:
        \begin{itemize}
        \item
            for every $s\in \ROARO$, $T$ halts on input $s$ and outputs
            an action $a$.
        \end{itemize}
    \item An \emph{extended environment} is a Turing machine $e$ such that:
        \begin{itemize}
            \item
            For every agent $T$, for every $s\in \ROA$,
            $e$ halts on input $\langle T,s\rangle$ and outputs
            a pair $\langle r,o\rangle$ where $r$ is a reward and $o$ is an observation.
        \end{itemize}
    \end{enumerate}
\end{definition}

There is a subtle nuance in Definition \ref{agentandenvironment}. Should the agent's
next action depend on the entire history (including prior actions), or only on prior
rewards and observations? One could argue that
the agent's next action needn't depend on its own past actions, since its own past actions
can be inferred from past rewards and observations.
In incentivizing self-awareness, it is convenient for the agent's
next action to formally depend on past actions. Perhaps this reflects that known
conscious agents (e.g.\ humans)
evidently do \emph{not} infer their own
past actions from remembered observations and rewards, but
remember the actions themselves, even if said memories are redundant.

\begin{definition}
\label{interactiondefn}
    Suppose $T$ is an agent and $e$ is an extended environment.
    The \emph{result of $T$ interacting with $e$} is the infinite
    reward-observation-action sequence
    \[\langle r_0,o_0,a_0,r_1,o_1,a_1,\ldots\rangle\]
    (each $r_i$ a reward, each $o_i$ an observation, and each $a_i$ an action)
    defined inductively as follows.
    \begin{itemize}
        \item $r_0$ and $o_0$ are obtained by computing $e$ on
        $\langle T,\langle\rangle\rangle$.
        \item $a_0$ is the output of $T$ on $\langle r_0,o_0\rangle$.
        \item For $i>0$, $r_i$ and $o_i$ are obtained by computing $e$
        on
        \[\langle T,\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1}\rangle\rangle.\]
        \item For $i>0$, $a_i$ is obtained by computing $T$ on
        \[\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1},r_i,o_i\rangle.\]
    \end{itemize}
\end{definition}

\begin{mylemma}
    For every agent $T$ and extended environment $e$, the result of $T$ interacting
    with $e$ (Definition \ref{interactiondefn}) is defined (all of the computations
    in question halt with the necessary outputs).
\end{mylemma}

\begin{proof}
    By a simultaneous induction:
    \begin{itemize}
        \item
        Each $r_i$ and $o_i$ are defined (and $r_i$ is a reward
        and $o_i$ is an observation) because, by induction,
        $\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1}\rangle$
        is defined and is in $\ROA$ (because, inductively,
        each $r_j$ is a reward, each $o_j$ is an observation,
        and each $a_j\in\mathbb N$ is an action, for all $j<i$)
        and thus $r_i$ and $o_i$ are defined with the correct form by
        Definition \ref{agentandenvironment} (part 2).
        \item
        Each $a_i$ is defined (and is an action) because, by induction,
        $\langle r_0,o_0,a_0,\ldots,r_i,o_i\rangle$
        is defined and is in $\ROARO$ (similar to the above) and thus
        $a_i$ is defined and is an action by Definition
        \ref{agentandenvironment} (part 1).
    \end{itemize}
\end{proof}

One important implication of extended environments is that they further divide
the (already divided) ways of measuring intelligence of RL agents. Intelligence
measures
\cite{alexander2019intelligence}
\cite{goertzel2006patterns} \cite{hernandez} \cite{legg2007universal}
which aggregate performance over traditional environments only measure
an agent's intelligence over those environments. The same measures could easily
be extended to also take extended environments into account, perhaps providing
measures which better capture agents' self-awareness and self-reflection abilities.

\section{Examples of Self-awareness-incentivizing Environments}
\label{basicexamplessection}

In this section, we give examples of extended environments which seem
to incentivize various forms of self-awareness. We are inspired by libraries of
traditional RL environments and other benchmarks \cite{bellemare2013arcade}
\cite{beyret2019animal} \cite{brockman2016openai} \cite{chollet2019measure}
\cite{cobbe2020leveraging}. All the environments in this section have a special
form: they always output rewards from $\{1,-1\}$ and they always output observation $0$.
In Section \ref{newextendedenvironmentsfromoldsecn},
this uniformity will allow all these examples to be generalized.
The examples in this section are implemented in our open-source
library of extended environments \cite{library}.

\begin{myexample}
\label{rewardagentforignoringrewardsexample}
    (Reward Agent for Ignoring Rewards)
    For each $s\in \ROARO$, let $s^0$ be the sequence equal to $s$ except that
    all rewards are $0$.
    We define an extended environment $e$ as follows
    (where $T$ is a Turing machine, $s\in \ROARO$, and $a$ is an action):
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0,0\rangle\\
        e(T,s\frown a)
        &= \langle r,0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a=T(s^0)$,}\\
            -1 & \mbox{if $a\not=T(s^0)$}
        \end{cases}
    \]
    (if $T$ does not halt on $s^0$ then $e$ does not halt on
    $\langle T,s\frown a\rangle$).
\end{myexample}

Example \ref{rewardagentforignoringrewardsexample} is implemented in
IgnoreRewards.py at \cite{library}.

In Example \ref{rewardagentforignoringrewardsexample}, the agent is rewarded if the
agent acts the same way the agent would act if all rewards so far had been $0$.
Otherwise, the agent is punished. Thus, paradoxically, the agent is rewarded for
ignoring rewards. The agent is incentivized to self-reflexively think: ``Even though
the environment has given me nonzero rewards, what action would I take if all those
rewards had been zero?'' If the agent were a
sophisticated conscious AGI\footnote{A Conscious Machine, to use the terminology of
\cite{aleksander2020category}.} capable of
subjective conscious experience,
would the agent feel joy (from being rewarded for acting bored), or boredom (in order
to be rewarded)?

\begin{mylemma}
\label{example1workslemma}
    Example \ref{rewardagentforignoringrewardsexample} really does define an
    extended environment.
\end{mylemma}

\begin{proof}
    Let $e$ be as in
    Example \ref{rewardagentforignoringrewardsexample}.
    We must show $e$ is an extended environment (Definition \ref{agentandenvironment}
    part 2). We must show that for each agent $T$ and each $s\in \ROA$,
    $e$ halts on $\langle T,s\rangle$ and outputs a pair $\langle r,o\rangle$
    such that $r$ is a reward and $o$ is an observation.

    \textbf{Case 1:} $s=\langle\rangle$. Then $e(T,s)$ halts with output
    $\langle 0,0\rangle$, so $r=0$ is a reward and $o=0$ is an observation.

    \textbf{Case 2:} $s\not=\langle\rangle$. By Lemma \ref{roaplaydecompositionlemma},
    $s=t\frown a$ for some $t\in \ROARO$ and action $a\in\mathbb N$.
    Since $t\in \ROARO$, clearly $t^0\in \ROARO$, therefore
    since $T$ is an agent, Definition \ref{agentandenvironment} (part 1)
    guarantees $T(t^0)$ is defined and is an action. It follows that the output
    $r$ in Definition \ref{rewardagentforignoringrewardsexample} is defined and is
    a reward (i.e., a rational number). And certainly the output $o=0$ is an
    observation (i.e., a natural number).
    So $e(T,t)$ outputs a pair $\langle r,o\rangle$ meeting the necessary
    requirements.
\end{proof}

For future examples, we will suppress the corresponding lemmas like
Lemma \ref{example1workslemma} which say that those examples really work.

Example \ref{rewardagentforignoringrewardsexample} is profound because it illustrates how,
in an extended environment, it is possible to give one sequence of rewards
in order to incentivize the agent to act as if a different sequence of rewards was given.
Imagine a film director who knows actors well enough to perfectly simulate them.
The director asks an actor to act perfectly bored.
The actor acts bored, and the director praises them for acting so bored.
The actor is happy to be praised, and ceases acting bored. So the director scolds
them for not acting bored. If this could continue for thousands of years, with the
director simulating the actor in order to tell how the actor would really act if
the actor were never praised or scolded, would the actor genuinely
become bored (since that would be the simplest way to satisfy the director)?

\begin{myexample}
\label{falsememoryexample}
    (False Memories)
    Suppose $s_0\in \ROA$. We define an extended environment
    $e$ as follows
    (with similar non-halting caveats as
    Example \ref{rewardagentforignoringrewardsexample}):
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0,0\rangle,\\
        e(T,s\frown a) &= \langle r,0\rangle,
    \end{align*}
    where
    \[
        r=
        \begin{cases}
            1 &\mbox{if $a=T(s_0\frown s)$,}\\
            -1 &\mbox{if $a\not=T(s_0\frown s)$}.
        \end{cases}
    \]
\end{myexample}

Example \ref{falsememoryexample} is implemented in FalseMemories.py at \cite{library}.

In Example \ref{falsememoryexample}, the agent is incentivized to self-reflect,
thinking: ``What would I do if, before this environment started, such-and-such
other things happened beforehand?'' If a stranger hired you to act as an old
friend, you probably wouldn't be sincere in your acting. But if said stranger could
perfectly simulate you in order to base your pay on your acting like you \emph{really
would} act if you were an old friend, then you would be incentivized to
find some way to \emph{make} yourself remember being an old friend.

You can imagine interacting with the agent in Example \ref{falsememoryexample} and
trying to convince them of the falsity of the false history. The agent would have
an incentive to resist your arguments. To quote Upton Sinclair,
``It is difficult to get a man to understand something, when his salary depends upon
his not understanding it!''

Henceforth, we will not explicitly mention the non-halting caveats in the remaining
examples.

\begin{myexample}
\label{backwardexample}
    (Backward Consciousness)
    We define an extended environment $e$ as follows.
    \begin{align*}
        e(T,\langle\rangle) &= \langle0,0\rangle,\\
        e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle)
        &= \langle r,0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a_n=T(r_n,o_n,a_{n-1},\ldots,r_1,o_1,a_0,r_0,o_0)$},\\
            -1 & \mbox{otherwise.}
        \end{cases}
    \]
\end{myexample}

Example \ref{backwardexample} is implemented in BackwardConsciousness.py at \cite{library}.

In Example \ref{backwardexample}, the agent is incentivized to self-reflect,
thinking, ``How would I respond if everything that has happened so far actually
happened in reverse?'' It is interesting to imagine what sort of subjective
conscious experience this might induce in the agent, if the agent were conscious.
Would the incentives eventually brainwash the agent into perceiving itself
moving backward through time\footnote{The difference between behaving as if 
the incentivized experience were its experience and actually subjectively 
experiencing that as its real experience brings to mind the objective misalignment 
problem presented in \cite{hubinger2019risks}. If an agent were to form an
idea of the experimenter's objective, would it be able to ``behave as if 
their objective were the same as the experimenter objective'' while maintaining its own 
objective or would it necessarily brainwash the agent into converging to the 
experimenter's objective? Is deception possible if the agent can be perfectly
simulated in an extended environment?}?

\begin{myexample}
\label{dejavuexample}
    (D\'{e}j\`{a} Vu)
    We define an environment $e$ as follows:
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0,0\rangle\\
        e(T,s\frown a) &= \langle r,0\rangle\\
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $T(s\frown a\frown s)=a$},\\
            -1 & \mbox{if $T(s\frown a\frown s)\not=a$}.
        \end{cases}
    \]
\end{myexample}

Example \ref{dejavuexample} is implemented in DejaVu.py at \cite{library}.

In Example \ref{dejavuexample},
the agent is incentivized to self-reflect and ask: ``Which action
would I take in order to ensure that I would take that same action if everything
which has happened so far were to repeat itself verbatim?''

\begin{myexample}
\label{incentivetoincentivizeexample}
    (Incentive to Incentivize)
    We define an environment $e$ as follows:
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0, 0\rangle\\
        e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle) &= \langle r, 0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $T(s')=0$},\\
            -1 & \mbox{if $T(s')\not=0$}
        \end{cases}
    \]
    where $s'=(r'_0,o'_0,a'_0,\ldots,r'_{n+1},o'_{n+1})$
    where $r'_0=0$, each $r'_{i+1}=a_i$,
    each $o'_i=0$, and each
    $a'_i=T(r'_0,o'_0,a'_0,\ldots,r'_i,o'_i)$.
\end{myexample}

Example \ref{incentivetoincentivizeexample} is implemented in
IncentivizeZero.py at \cite{library}.


In Example \ref{incentivetoincentivizeexample}, the agent is tasked with choosing
rewards in such a way that if those rewards were fed to a simulated copy of the agent,
then the simulated copy would take action $0$.
Thus, the agent is incentivized to choose rewards by self-reflecting:
``Which rewards would do the best job of compelling me to take action $0$ as often
as possible?'' We might imagine the agent playing a video-game in which he sees
a cartoon of himself
in front of a keyboard. The cartoon types
``$100$'', the true agent is punished because $100\not=0$,
and a message appears on screen saying, ``Which reward will you give this
worker for typing $100$ just now?'' The
agent responds by choosing some reward, and sees an animation
of the reward being given to the cartoon. The cartoon
then types ``$0$'', and immediately the true agent is rewarded for getting the cartoon
to type $0$. Then a message appears, saying, ``Which reward will you give this
worker for typing $0$ just now?'' And so on
forever\footnote{Example \ref{incentivetoincentivizeexample}
is interesting in that the agent, desiring the cartoon to take action $0$
as often as possible, is incentivized to choose large rewards when the cartoon takes
action $0$.
If rewards are limited to $\mathbb Q$, then the agent faces a dilemma
similar to one in RL cancer treatment applications.
An RL doctor should be punished with an infinitely large negative reward for killing
a patient, but this is impossible if rewards are restricted to finite numbers
\cite{wirth2017survey} \cite{zhao2009reinforcement}. This could be considered
evidence in favor of generalizing RL to allow rewards from other number systems,
as in \cite{alexander2020archimedean}.}.

Many other simple and interesting examples could be given. For example:
\begin{itemize}
    \item
    An extended environment could reward agents based on how many steps\footnote{To quote
    Gavane: ``The problem is that the response-times-dependent performance of an agent
    is not properly reflected in [Hern{\'a}ndez-Orallo and Dowe's] intelligence test,
    since the simulated environments remain unaware of the response times of agents,
    with the result that the perceptions of an agent are still independent of its
    response times'' \cite{gavane}.} the agent takes. For example, in
    RuntimeInspector.py at \cite{library}, we implement environments that punish
    agents for running too fast or too slow.
    \item
    An extended environment could reward agents based on how much memory
    they use to compute each action\footnote{In some sense, by giving the environment
    access to the agent's source-code, we allow the environment to reflect the agent's
    own internal signals. Thus, extended environments generalize the idea of
    agents modified to manually predict their own internal signals, as in
    \cite{sherstan2016introspective}.}.
    \item
    If agents are allowed to be non-deterministic, then an extended environment
    could estimate how deterministic an agent is (e.g., by repeatedly simulating
    the agent on the same history and seeing whether or not the agent outputs the same
    action every time). We implement such environments in
    DeterminismInspector.py at \cite{library}.
\end{itemize}
We do not intend the examples in this section to be exhaustive.

\section{New extended environments from old}
\label{newextendedenvironmentsfromoldsecn}

In this section, we will generalize the examples from Section \ref{basicexamplessection}.

\begin{definition}
\label{handicapdefn}
    (Handicaps)
    \begin{enumerate}
        \item
        An extended environment is \emph{merciful} if it never outputs negative rewards.
        \item
        By a \emph{handicap}, we mean an extended environment which always outputs $0$ as
        observation and always outputs either $1$ or $-1$ as reward.
        \item
        If $e$ is a merciful extended environment and $h$ is a handicap,
        we define a new environment $e*h$ as follows:
        \[
            (e*h)(T,s) =
            \begin{cases}
                \langle r_e, o_e\rangle &\mbox{if $r_h=1$},\\
                \langle -1, o_e\rangle &\mbox{if $r_h=-1$},
            \end{cases}
        \]
        where $e(T,s)=\langle r_e,o_e\rangle$ and $h(T,s)=\langle r_h,0\rangle$.
    \end{enumerate}
\end{definition}

Intuitively, $e*h$ is just like $e$ except that $h$ imposes an additional constraint
on the agent. Any time the agent violates that constraint, the agent is punished,
and forfeits any reward that would otherwise have been won from $e$. Aside from the
pain caused by $h$, the agent otherwise observes $e$ unaltered (that is, the observations
from $e$ are not changed). We require $e$ to be merciful in order that large negative
rewards from $e$ do not confuse the intended incentive, for if the agent could avoid
a larger punishment by intentionally using the handicap, then it would not
be much of a handicap. The requirement that $e$ be
merciful could be weakened if we revised the RL framework
to allow infinitary rewards, as in \cite{alexander2020archimedean}.

Definition \ref{handicapdefn} is implemented in Handicap.py in \cite{library}.

\begin{myexample}
    For any merciful extended environment $e$, each example $h$ in
    Section \ref{basicexamplessection} can be applied as a handicap, yielding a
    version $e'=e*h$ of $e$ modified to incentivize the corresponding type of self-awareness.
    \begin{itemize}
        \item
        Modifying $e$ using Example \ref{rewardagentforignoringrewardsexample}
        (``Reward Agent for Ignoring Rewards'')
        yields a version of $e$ where the agent is penalized for taking actions in
        response to nonzero rewards. The agent is incentivized to strategically
        take non-bored actions (for which it receives small penalties) in order to
        put itself in a position where its next bored action coincidentally is an
        action which wins a large enough reward from $e$ to make up for said
        penalties.
        \item
        Modifying $e$ using Example \ref{falsememoryexample}
        (``False Memories'') yields a version of $e$ where the agent is penalized
        whenever it acts inconsistently with a fictitious history. The agent is
        incentivized to strategically choose when to act so inconsistently so that
        a later action, consistent with said false history, happens to win a large
        reward from $e$. For example, the agent hired to act as an old friend
        might strategically choose to abandon its employer (an action inconsistent
        with old friendship) during a time of peace, so as to be able to
        swoop in and save the day (an action consistent with old friendship)
        during a time of crisis.
        \item
        Modifying $e$ using Example \ref{backwardexample}
        (``Backward Consciousness'') would yield a version of $e$
        where the agent must act as if time is reversed, or else suffer punishment.
        The agent can strategically choose to accept some punishment, acting other
        than it would act if time really were reversed, in order to get into a state
        where subsequently acting as if time is reversed will yield more reward.
        \item
        Modifying $e$ using Example \ref{dejavuexample} (``D\'{e}j\`{a} Vu'')
        would yield a version of $e$ where the agent is incentivized
        to act as if
        everything (including said action) had all happened before.
        The agent can strategically choose to not so act (thus suffering some
        pain) in order to get to a state where it is easier to so act and
        to gain rewards from $e$ by so acting.
        % \item
        % Modifying $e$ using Example \ref{incentivetoincentivizeexample}
        % (``Incentive to Incentivize'') would yield an environment in which
        % the agent watches a copy of itself interacting with $e$, and has to
        % guide that copy by choosing rewards to give to it, in such a
        % way as to incentivize the copy to take prescribed actions. When the copy
        % takes the prescribed action, the true agent enjoys whatever rewards
        % $e$ would give for that action. The agent can strategically guide the
        % copy to take non-prescribed actions (this will cause the agent to suffer
        % pain), so as to get the copy into a position where subsequently taking
        % prescribed actions will yield more reward.
    \end{itemize}
\end{myexample}


\section{Abstract examples}

In this section, we give some examples of a more abstract nature.
Having read the previous sections, we assume the reader is now
sufficiently familiar with the formal framework that we can describe
the following examples informally.

\subsection{Identifying with a character}

\begin{quote}
    ``They've been there since childhood, fixed in the same place,
    with their necks and legs fettered, able to see only in front of
    them, because their bonds prevent them from turning their
    heads around. (...) Do you suppose, first of all, that these
    prisoners see anything of themselves and one another besides
    the shadows that the fire casts on the wall in front of
    them?''---Plato \cite{republic}.
\end{quote}

In the next example,
we will finally make some nontrivial usage of observations. Using carefully chosen
observations, we will incentivize the agent, who might be thought of as
controlling a character on a video-game screen,
to ``suspend disbelief'' and self-identify with that character.

\begin{myexample}
\label{selfinsertionexample}
Assume $e$ is an environment. We define a new environment $e'$ as follows.
In response to any $s\in \ROA$, $e'$ computes a reward and observation as
follows. First, $e'$ plugs $s$ into $e$ to see which reward-observation pair
$\langle r,o\rangle$ would be returned by $e$ in response to $s$.
Then, using a suitable bijective
pair-encoding function $\ulcorner\bullet\urcorner$
(for example Cantor's pairing function), $e'$
encodes $\langle r,o\rangle$ into a single natural number
$o'=\ulcorner\langle r,o\rangle\urcorner$. If $s$ is empty, then $e'$
returns reward $r'=0$ and observation $o'$. Otherwise,
$e'$ computes a modified version $t$ of $s$ by replacing every
reward-observation pair $\ldots,r'_i,o'_i,\ldots$ in $s$ by
the projections $\ldots r_i,o_i\ldots$
projected by $o'_i=\ulcorner\langle r_i,o_i\rangle\urcorner$.
Finally, $e'$ lets $r'=1$ if $T(t)=a$ (where $a$ is the last action
in $s$), or $r'=-1$ otherwise, and outputs reward $r'$ and observation $o'$.
\end{myexample}


In Example \ref{selfinsertionexample}, one might imagine $e'$ as a room containing nothing
but an arcade game $e$. There is nothing for the agent in the room to do
except play this arcade game.
When played, the arcade game
visually displays rewards, but the agent merely observes them, and does not
``feel'' them. However, the agent is rewarded for acting as if actually feeling
those displayed rewards, and punished for not so acting.
In this way, the agent is incentivized
to self-identify with the protagonist in the video-game, self-reflexively asking,
``Which action would I take if I actually \emph{felt}
those rewards which I \emph{see} on the screen?''

An abstract implementation of Example \ref{selfinsertionexample} is available in
abstract/SelfInsert.py
in \cite{library}, except that there, in order to avoid the encoding details,
we give an implementation in which observations are allowed to
literally be pairs.

\subsection{Playing in the mirror}

\begin{quote}
    ``I may add that when a few days under nine months old he associated his own name with
    his image in the looking-glass, and when called by name would turn towards the glass
    even when at some distance from it.''---Charles Darwin \cite{darwin1877biographical}
\end{quote}

It has been suggested \cite{lacan} that recognizing oneself in the mirror is linked
to the development of certain parts of the human psychology. Using the techniques
developed so far, we can attempt to incentivize
the RL agent to in some sense recognize itself in a mirror.

\begin{myexample}
Suppose $e$ is a merciful environment whose observations encode snapshots of a room.
Assume the room contains a mirror, and assume the room is laid out in such a way that
everything important in the room is visible in the mirror (assume
the environment constrains the agent to never look away from
the mirror). We could derive an extended environment $e'$ which shows
the same observations as $e$ and gives the same rewards,
except it punishes the agent for acting differently than the agent
would act if the agent \emph{only} observes the mirror.
To make this precise, for any $s=(r_0,o_0,a_0,\ldots,r_n,o_n)\in \ROARO$ produced
by $e'$, and any action $T(s)=a_n$, we would say that ``$a_n$ is as if the
agent only observes the mirror''
if $T(s')=a_n$, where $s'=(r_0,o'_0,a_0,\ldots,r_n,o'_n)$,
where each $o'_i$ is $o_i$ cropped to only the include the mirror.
\end{myexample}

To make this even more elaborate, the $o'_i$ in the above example could be further
modified by adding an image of the agent's ``body'' into the mirror. For example,
the agent's ``body'' shown in $o'_i$
might be a visualization systematically derived from the steps which the Turing
machine $T$ performed in the computation of $T(r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1})$.
These computation steps would not be available to a traditional RL environment, but they are
available to an extended environment because of the inclusion of $T$ itself as an
argument passed to the extended environment.

\subsection{Binocular vision}

\begin{quote}
    ``...as there are two eyes, so there may be in the soul
    something analogous, that of the eyes, doubtless, some one organ is formed, and
    hence their actualization in perception is one...''---Aristotle
    \cite{aristotlesense}
\end{quote}

Humans seem to consciously perceive a three-dimensional model of their surrounding
world, even though the raw data which we actually receive consists of two two-dimensional
image-feeds (one for each eye). The following example is intended to incentivize an RL
agent to learn to perceive the world through binocular vision like a human.

\begin{myexample}
\label{binocularexample}
    Suppose $V$ is a video game intended to be played on a virtual-reality headset,
    so at any moment during the game, $V$ produces two snapshots, one for the player's
    left eye, one for the player's right eye. Assume the player is constrained in $V$
    so as never to be able to put their eyes into weird configurations (such as
    the weird configurations in
    \cite{gallagher2020third}): thus, at any moment, the two snapshots $s_1,s_2$
    which $V$ is
    displaying to the player are equivalent to a single 3-D matrix encoding
    the model $m(s_1,s_2)$ which the player is intended to perceive (with cells blanked
    out where the player's vision is obstructed by obstacles). Let $e$ be the extended
    environment whose observations encode pairs $\langle s_1,s_2\rangle$ of snapshots
    displayed by $V$ in response to the player pressing keys encoded by the agent's
    actions. In response to history $(r_0,o_0,a_0,\ldots,r_n,o_n,a_n)\in \ROA$ (where
    each $o_i$ encodes $\langle s^i_1,s^i_2\rangle$), let $r_{n+1}=1$ if
    $a_n=T(r_0,o'_0,a_0,\ldots,r_n,o'_n)$, where each $o'_i$ encodes
    $m(s^i_1,s^i_2)$, otherwise let $r_{n+1}=-1$.
\end{myexample}

In Example \ref{binocularexample}, upon being presented a sequence of pairs of 2-D snapshots,
the agent is rewarded for acting as if it had instead been presented with an equivalent
sequence of 3-D models\footnote{To quote Yampolskiy:
``With illusions, it is possible to set up a test in which it is only by experiencing
an illusion that the agent is able to enter into a certain internal state, which
we can say it experiences'' \cite{yampolskiy2017detecting}.
The subjective experience of depth perception is certainly an illusion,
although, just like the sky's blue color,
``we don't consider it to be an illusion because we experience it
so frequently'' (ibid).}.
The agent is incentivized to self-reflectively ask, ``Which action would I take if instead
of observing those 2-D snapshot-pairs, I observed equivalent 3-D models?''

An abstract implementation of Example \ref{binocularexample} (where the necessary camera
functions are delegated to the user) is available in abstract/BinocularVision.py
in \cite{library}.

\subsection{Nature and Nurture}

\begin{quote}
    ``If only one soul was created, and all human souls are descended from it,
    who can say that he did not sin when Adam sinned?''---Augustine \cite{augustine1993free}
\end{quote}

The following example is motivated by biological considerations.
The programming which determines an organism's behavior is largely inherited.
As an extreme simplifying assumption, we could consider asexual organisms
which perfectly inherit their agency-functions from their lone parents without
any mutations, in which case the child, in some sense, has the exact same
source-code as the parent. As the parent interacts with the child, then, it is,
in some sense, interacting with an extended environment.

\begin{myexample}
\label{cryingbabyexample}
    (The Crying Baby)
    We define an extended environment $e$ as follows.
    \begin{itemize}
        \item
        Considered as actions taken by an adult
        (and also as observations seen by a baby), let $0$ denote ``feed the baby''
        and let all naturals $>0$ denote ``don't feed the baby''.
        \item
        Considered as actions taken by a baby
        (and also as observations seen by an adult), let $0$ denote ``laugh'' and let
        all naturals $>0$ denote ``cry''.
        \item
        We define a \emph{nutrition}
        function $N:\ROA\to \mathbb N$
        by $N(s)=100+25f(s)-\mbox{len}(s)$ where $f(s)$ is the number of
        times that the action ``feed the baby'' is taken in $s$,
        and $\mbox{len}(s)$ is the length of $s$.
        \item
        Let $e(T,\langle\rangle)=\langle 1,\mbox{``laugh''}\rangle$.
        \item
        For each $\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle\in \ROA$,
        let
        \[
            e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle)
            =
            \langle r,o\rangle
        \]
        where $r$ and $o$ are defined as follows.
        For each $i=0,\ldots,n$, define
        \begin{align*}
            r'_i &=
                \begin{cases}
                    1 &
                    \mbox{if
                    $50\leq N(\langle r_0,o_0,a_0,\ldots,r_i,o_i,a_i\rangle)\leq 200$,}\\
                    -1 & \mbox{otherwise,}
                \end{cases}\\
            o'_i &= a_i\\
            a'_i &= T(\langle r'_0,o'_0,a'_0,\ldots,r'_i,o'_i\rangle).
        \end{align*}
        Define $o=a'_n$ and
        \[
            r =
            \begin{cases}
                1 & \mbox{if $a'_n=\mbox{``laugh''}$,}\\
                -1 &\mbox{if $a'_n=\mbox{``cry''}$.}
            \end{cases}
        \]
    \end{itemize}
\end{myexample}

In Example \ref{cryingbabyexample}, rather than mentally self-reflecting,
the agent is physically self-reflected into the form of a newborn baby
who has inherited the agent's own source-code. Despite having the same
internal source-code $T$ (``nature''), the agent and the baby behave
differently because they perceive the environment differently (``nurture'').
The baby's objective is to maintain satiation within a satisfying range,
whereas the agent's objective is to pacify the baby.

Using the same basic idea, one could extend Example \ref{cryingbabyexample}
into an example modelling an entire society of interacting people of different
types\footnote{Similar to how Plotinus compares society to an elaborate play
where the actors ``define their own rewards and punishments because
they themselves assist in the rewards and punishments'' \cite{plotinus}.},
all sharing the same internal ``nature'' ($T$).

Example \ref{cryingbabyexample} is implemented in CryingBaby.py at \cite{library}.

\section{Examples involving self-recognition}

\begin{myexample}
\label{selfrecognitionexample}
    (Incentivizing Self-recognition)
    Let $s_0,s_1,\ldots$ be a canonical computable enumeration of
    the nonempty sequences of $\ROA$.
    We define an environment $e$ as follows:
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0, s_0\rangle\\
        e(T,s\frown a) &= \langle r, s_n\rangle
    \end{align*}
    where $n=\frac{\mbox{len}(s\frown a)}{3}$ is the number
    of actions in $s\frown a$ and where
    \[
        r =
        \begin{cases}
            1 &\mbox{if $a>0$ and $a'=T(s')$,}\\
            1 &\mbox{if $a=0$ and $a'\not=T(s')$,}\\
            -1 &\mbox{otherwise}
        \end{cases}
    \]
    where $s_{n-1}=s'\frown a'$.
\end{myexample}

In Example \ref{selfrecognitionexample}, the agent is systematically
shown all non-empty histories in $\ROA$, and for
each one, the agent either types ``Looks like me'' (any action $>0$)
or ``Doesn't look like me'' ($0$). When shown
$s'\frown a'$, the agent is rewarded if and only if the agent
correctly determines whether or not $a'$
is the action the agent itself would take in response to $s'$.
Thus, the agent is incentivized to self-reflect in order to ask, ``If I were prompted
by that history, would I act that way?''

The following example is partly motivated by \cite{yampolskiy2012ai}.

\begin{myexample}
\label{otheraspectsexample}
    (Recognizing other aspects of oneself)
    All the below environments are similar to Example \ref{selfrecognitionexample},
    and we describe them informally to avoid technical details.
    \begin{itemize}
        \item
        (Supervised learning)
        Assume there is a canonical, computable function $f$ which transforms
        each RL agent $A$ into a supervised learning agent $f(A)$. By a \emph{supervised
        learning trial} we mean a quadruple $\langle L,T,I,p\rangle$ where $L$ is a finite set
        of labels, $T$ is a sequence of images with labels from $L$ (a \emph{training set}),
        $I$ is an unlabeled image, and $p:L\to \mathbb Q\cap [0,1]$ is a function
        assigning to each label $\ell\in L$ a probability that $\ell$ is the correct label
        for $I$. We define an extended environment as follows.
        The agent $A$ is sytematically shown all supervised learning trials and must
        take action $>0$ (``Looks like me'') or $0$ (``Doesn't look like me''), and is
        rewarded or punished accordingly, depending whether or not $f(A)$ would
        output $p$ in response to $I$ after being trained with $T$.
        \item
        (Unsupervised learning)
        Assume there is a canonical, computable function $g$ which transforms each RL
        agent $A$ into an unsupervised learning agent $g(A)$.
        By an \emph{unsupervised learning trial} we mean a triple
        $\langle n,D,C\rangle$ where $n$ is a positive integer, $D\subseteq \mathbb Q^n$
        is a finite set of $n$-dimensional points with rational coordinates, and $C$
        is a clustering of $D$.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all unsupervised learning trials and must take action $>0$ (``Looks like me'')
        or $0$ (``Doesn't look like me''), and is rewarded or punished
        accordingly, depending
        whether or not $g(A)$ would cluster $D$ into clustering $C$.
        \item
        (The Turing Test)
        Assume there is a canonical, computable function $h$ which transforms each RL
        agent $A$ into an English-speaking chatbot $h(A)$.
        By a \emph{chatbot trial} we mean a sequence of strings of English characters.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all chatbot trials and must take action $>0$ (``Looks like me'') or $0$
        (``Doesn't look like me''), and is rewarded or punished accordingly, depending
        whether or not even-numbered strings in the chatbot trial are what $h(A)$ would
        say in response to the user saying the odd-numbered strings.
        \item
        (Adversarial sequence prediction) Similar to the above environments, assuming
        a canonical computable function which transforms each RL agent into a predictor
        in the game of adversarial sequence prediction \cite{hibbard2008adversarial}
        \cite{hibbard}.
        \item
        (Mechanical knowing agent) Assume there is a canonical, computable function
        $i$ which transforms each RL agent $A$ into a code $i(A)$ of a computably
        enumerable set of sentences in the language of Epistemic Arithmetic
        \cite{shapiro}; $i(A)$ is thought of as a mechanical knowing agent
        \cite{carlson}. We define an extended environment as follows. The agent $A$
        is systematically shown all sentences in the language of Epistemic Arithmetic,
        with each sentence being repeated infinitely often.
        Upon being shown sentence $\phi$ for the $n$th time, the agent must take action
        $>0$ (``I know $\phi$ is true'')
        or $0$ (``I'm not sure if $\phi$ is true'')
        and is rewarded if and only if $\phi$ is
        enumerated by $i(A)$ in $\leq n$ steps of computation.
    \end{itemize}
\end{myexample}

The extended environments in Example \ref{otheraspectsexample} incentivize the agent
to self-reflect, asking itself questions like:
\begin{itemize}
    \item
    ``Is that how I would
    classify that image, given that training set?''
    \item ``Is that how I would cluster that set of points?''
    \item ``Is that what I would say in response to that conversation?''
    \item ``Are those the adversarial sequence predictions I would make?''
    \item ``Does that mechanical knowing agent know the same things I know?''
\end{itemize}
That perceptive non-RL agents such as supervised learning agents, unsupervised learning agents,
etc., nevertheless might have some connection to RL-style reward mechanisms
is suggested by Aristotle:
``Whatever has a sense has the capacity for pleasure and pain and therefore has pleasant
and painful objects present to it...'' \cite{aristotlesoul}.

\section{Measuring Self-awareness}

As an application, self-awareness-incentivizing extended environments could be used
to quantify
RL agents' self-awareness.
In order to measure the self-awareness of a specific agent (whose source-code
is available to us), we could run the agent against a battery
of extended environments that incentivize self-awareness, and see how the agent performs.
Ideally, the battery of extended environments should be fairly large and contain many
different types of environments, since an agent could perform poorly (resp.\ well)
in a smaller set of environments by chance, despite being quite
self-aware (resp.\ self-unaware) in general\footnote{In the same way, computer programs can
perform well on IQ tests despite being dumb \cite{sanghi2003computer}.}.
The larger and more representative the battery of extended environments, the better
the measurement of self-awareness would be.
One way to obtain a large battery of environments would be to start with
a battery of non-extended environments, and a separate body of handicap
extended environments (as in Section \ref{newextendedenvironmentsfromoldsecn}),
and exhaustively apply all the different handicaps to all the different
non-extended environments. In \cite{library}, in the file
AwarenessBenchmark.py, we offer a function for doing exactly this.

This technique could be useful for obtaining
empirical insight into questions about the self-awareness, or lack thereof, of
various entities. For example, in order to get some empirical insight into the
self-awareness, or lack thereof, of an NLP system like GPT-3
\cite{chalmers}, one could twist that system into an RL agent (by means of template
programming) and see how well said RL agent performs in extended environments that
incentivize self-awareness.

It might even be possible to use extended environments to test the self-awareness
of living creatures such as lab mice. At least it would be, if a given species of
lab mice were entirely ``nurture'', as opposed to ``nature''. What we mean by a
species being entirely ``nurture'' is that any two members of that species,
if raised through identical life circumstances, would act similarly in identical environments.
Thus, if a species were entirely nurture, and if two specimens were born in identical
rooms, and experienced identical or near-identical lives\footnote{Possibly even
while still in the womb. To quote Plato: ``But it's hardly surprising you haven't
heard of these athletics of the embryo. It's a curious subject, but I'd like to
tell you about it'' \cite{platolaws}.},
and were then placed in identical mazes, then they would perform identically.
Given such a species, it would be possible to see how members perform in extended
environments, albeit perhaps at great expense. One would have to carefully raise many
similar specimens through identical or near-identical lives, diverging only at key
points in order to compute the underlying $T$ action-function on different histories.
By seeing how the underlying $T$ agent performs on well-chosen extended environments,
we could get an idea of how self-aware those specimens were.


\bibliographystyle{plain}
\bibliography{intro}

\end{document}