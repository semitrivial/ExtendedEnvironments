\documentclass[runningheads]{llncs}
\usepackage{amsfonts}
\usepackage{amsmath}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{myquestion}[mytheorem]{Question}
\newtheorem{myexample}[mytheorem]{Example}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{mycorollary}[mytheorem]{Corollary}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myprinciple}[mytheorem]{Principle}

\begin{document}

\title{Extending environments to incentivize self-reflection
in reinforcement learning}
\titlerunning{Extending environments to incentivize self-reflection}

\author{Samuel Allen Alexander\inst{1}\orcidID{0000-0002-7930-110X}}
\authorrunning{S.\ A.\ Alexander}
\institute{The U.S.\ Securities and Exchange Commission
\email{samuelallenalexander@gmail.com}
\url{https://philpeople.org/profiles/samuel-alexander/publications}}

\maketitle

\begin{abstract}
    We consider an extended notion
    of reinforcement learning environment, in which the environment is able
    to simulate the agent. We give
    examples of some such extended environments which seem to incentivize
    various different types of self-reflection or self-awareness.
    These environments are not particularly useful in themselves, but we
    hope they might serve to guide the development of self-aware reinforcement
    learning agents, as well as to help measure the degree to which existing
    reinforcement learning agents are or are not self-aware; to that end,
    we have released an open-source library of such environments.
    We also speculate
    about subjective conscious experiences which
    might be incentivized in self-aware reinforcement learning agents
    placed within these extended environments.
\end{abstract}

\section{Introduction}

In order to motivate the environments in this paper (and in the accompanying
open-source library \cite{library} which we are publishing alongside the
paper),
we begin with three thought experiments.
\begin{itemize}
    \item (Oracular Obstacle Courses)
        An ordinary obstacle course might depend on what you \emph{do}:
        step on a button and spikes appear, for example.
        But imagine an obstacle course which depends on what you
        \emph{would hypothetically do}:
        you walk into a room with no button, but spikes appear if you
        \emph{would} hypothetically step on the button if there was one.
    \item (Treasures for the Worthy\footnote{This thought experiment bears some
        similarity to Newcomb's Paradox \cite{nozick1969newcomb}.})
        You wander through rooms, each containing a treasure chest.
        Most of the rooms also contain a guard. In a room with no guard, you
        may optionally choose to take the treasure, which acts as a free reward.
        In a guarded room, you may optionally try to take the treasure.
        If you do so, the guard will determine whether or not you would hypothetically
        take the treasure if the room were unguarded. If you would take the treasure
        if the room were unguarded, then the guard blocks the treasure and zaps you.
        But if you would leave the treasure alone if the room were unguarded, then
        the guard lets you take the treasure.
    \item (The Clever Employer)
        Your boss demands that you act as if you're paid \$1000 per hour.
        But instead of paying you \$1000 per hour, here's what he does instead.
        Every hour, if you acted exactly as if you're paid \$1000 per hour,
        he pays you \$100. But every hour, if you did not act exactly as if you're
        paid \$1000 per hour, then he pays you nothing, and charges you a \$100
        penalty instead.
\end{itemize}

These thought experiments would be hard to perform on a human subject, because they
require knowing what the human subject would hypothetically do in various
counterfactual circumstances. But if the experiment administrator could simulate
a perfect copy of you, then the experiments would be doable. In the first two
experiments, would you eventually figure out the pattern? In the last experiment,
would you eventually begin to believe you were really being paid \$1000 per hour?

This is a paper about reinforcement learning (RL).
In RL, agents interact with environments, where they take actions and
receive rewards and observations in response to those actions.
For sake of simplicity, we restrict our attention
to deterministic environments and deterministic agents, but the basic idea
would easily adapt
to non-deterministic RL.

Although the details differ between authors, essentially,
an RL environment is a Turing machine (i.e., a computer program)
$e$ which outputs a reward-observation pair
\[
  \langle r,o\rangle=e(r_0,o_0,a_0,\ldots,r_n,o_n,a_n)
\]
in response to a
reward-observation-action sequence $(r_0,o_0,a_0,\ldots,r_n,o_n,a_n)$.
An RL agent is a Turing machine $T$ which outputs an action
\[
  a=T(r_0,o_0,a_0,\ldots,r_n,o_n)
\]
in response to a
reward-observation-action-$\cdots$-reward-observation sequence.
An RL agent can interact with an RL environment in an obvious way.
However, there is another type of environment in which
RL agents can interact just as well. We define an \emph{extended
environment} to be a Turing machine $e$ which outputs a reward-observation pair
\[
    \langle r,o\rangle=e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n \rangle)
\]
in response
to a reward-observation-action sequence
along with an RL agent $T$.
Intuitively, this should be thought of as follows: when the agent enters the environment,
the environment is made aware of the agent's source-code, and can use that
source-code to simulate the agent when computing rewards and observations.

For example, the three thought experiments in the beginning of this Introduction could
be cast as extended environments in which the environment somehow knows the agent's
source-code and uses that to determine what the agent would do in various
hypothetical situations.
Clearly such
extended environments are not the sort of environment RL agents are traditionally
intended to interact with\footnote{Such environments might, however,
accidentally arise if both environment
and agent are implemented on the same machine and the environment is managed by an AI
sophisticated enough to exploit unintended informational side channels, as in
\cite{yampolskiy2012leakproofing}.}. However, such environments could be
useful on the path to Artificial
General Intelligence (AGI) because they seem to incentivize self-awareness:
in order to figure out the environment, the agent is incentivized to look within
and ask, ``How would I act in such-and-such alternative scenarios?''

One might try to imitate an extended environment with a traditional environment by
backtracking---rewinding the environment itself to a prior state after seeing how the
agent performs along one path, and then sending the agent along a second path.
But the agent itself would retain memory of the first path, and the agent's decisions
along the second path might be altered by said memories. Thus the result would not be
the same as immediately sending the agent along the second path while secretly simulating
the agent to determine what it would do if sent along the first path.

We will give examples of
extended environments designed to incentivize RL agents to
recursively engage in self-awareness in various ways.
It is an interesting question how traditional RL agents would perform
in these extended environments. We ran a DQN agent on some of these
extended environments and found that it performs
better than we initially expected on some environments; more on this below.
We tentatively conjecture that the ability of traditional RL agents to cope with these
extended environments is related to two things. First, how far the actual history differs from
the hypothetical histories which the environment examines. Second, the size of the action space. 

For example, in the above thought experiment of ``Treasures for the Worthy'',
the guards base their decisions on a hypothetical history which is identical
with the true history except for just their own room, and so, since the
two histories are so similar, the agent might be able to determine that
uniformly refusing to take unguarded treasures is better than uniformly
always taking unguarded treasures, even without understanding why that is so.
If the hypothetical history is more drastically different than the true
history, more self-awareness might be needed in order for the agent to
choose decent actions. Similarly, any other extension of the environment that 
modifies the history can be \emph{solved} by continuously playing the same 
action across a variety of circumstances. For games with small action spaces, like 
Guarded Treasure, it's very likely the agent will stumble upon a solution that 
just plays the same action across a variety of circumstances and exploit it.

We hope these examples will facilitate
new self-aware RL techniques, hopefully as a step toward AGI.

Alongside this paper, we are also releasing an open-source library
\cite{library} of extended environments
implemented in python. The library also includes some machinery for running an
agent against a battery of extended environments in order to empirically
probe the agent's self-awareness (or lack thereof).

\section{Preliminaries}

Throughout the paper, $\frown$ denotes concatenation.

\begin{definition}
\label{historiesdefn}
    \begin{enumerate}
        \item
        By a \emph{reward}, we mean a rational number.
        By an \emph{observation}, we mean a natural number.
        By an \emph{action}, we mean a natural number.
        \item
        By $(ROA)^*$ we mean the set of all finite sequences which begin with
        a reward, end with an action, and follow the pattern
        ``reward, observation, action, ...''. We also include
        the empty sequence $\langle\rangle$ in this set.
        \item
        By $(ROA)^*RO$ we mean the set of all sequences of the form
        $s\frown r\frown o$ where $s\in (ROA)^*$, $r$ is a reward, and
        $o$ is an observation.
    \end{enumerate}
\end{definition}

In Definition \ref{historiesdefn} part 3, the intuition is that in response to
history $t$, an environment rewards an agent with reward $r$ and
shows the agent an observation $o$.

\begin{lemma}
\label{roaplaydecompositionlemma}
    If $s\in (ROA)^*$, then either $s=\langle\rangle$, or else
    $s=t\frown a$ for some $t\in (ROA)^*RO$ and action $a\in\mathbb N$.
\end{lemma}

\begin{proof}
    Trivial. \qed
\end{proof}

In Lemma \ref{roaplaydecompositionlemma}, when
$s=t\frown a$, the intuition is that an agent, having been prompted to act by
history $t$, responds by taking action $a$.

\begin{definition}
\label{agentandenvironment}
(Agents and environments)
    \begin{enumerate}
    \item An \emph{agent} is a Turing machine $T$ such that:
        \begin{itemize}
        \item
            for every $s\in (ROA)^*RO$, $T$ halts on input $s$ and outputs
            an action $a$.
        \end{itemize}
    \item An \emph{extended environment} is a Turing machine $e$ such that:
        \begin{itemize}
            \item
            For every agent $T$, for every $s\in (ROA)^*$,
            $e$ halts on input $\langle T,s\rangle$ and outputs
            a pair $\langle r,o\rangle$ where $r$ is a reward and $o$ is an observation.
        \end{itemize}
    \end{enumerate}
\end{definition}

There is a subtle nuance in Definition \ref{agentandenvironment}. Should the agent's
next action depend on the entire history (including prior actions), or only on prior
rewards and observations? One could argue that
the agent's next action needn't depend on its own past actions, since its own past actions
can be inferred from past rewards and observations.
In incentivizing self-awareness, it is convenient for the agent's
next action to formally depend on past actions. Perhaps this reflects that known
conscious agents (e.g.\ humans)
evidently do \emph{not} infer their own
past actions from remembered observations and rewards, but
remember the actions themselves, even if said memories are redundant.

\begin{definition}
\label{interactiondefn}
    Suppose $T$ is an agent and $e$ is an extended environment.
    The \emph{result of $T$ interacting with $e$} is the infinite
    reward-observation-action sequence
    \[\langle r_0,o_0,a_0,r_1,o_1,a_1,\ldots\rangle\]
    (each $r_i$ a reward, each $o_i$ an observation, and each $a_i$ an action)
    defined inductively as follows.
    \begin{itemize}
        \item $r_0$ and $o_0$ are obtained by computing $e$ on
        $\langle T,\langle\rangle\rangle$.
        \item $a_0$ is the output of $T$ on $\langle r_0,o_0\rangle$.
        \item For $i>0$, $r_i$ and $o_i$ are obtained by computing $e$
        on
        \[\langle T,\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1}\rangle\rangle.\]
        \item For $i>0$, $a_i$ is obtained by computing $T$ on
        \[\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1},r_i,o_i\rangle.\]
    \end{itemize}
\end{definition}

\begin{lemma}
    For every agent $T$ and extended environment $e$, the result of $T$ interacting
    with $e$ (Definition \ref{interactiondefn}) is defined (all of the computations
    in question halt with the necessary outputs).
\end{lemma}

\begin{proof}
    By a simultaneous induction:
    \begin{itemize}
        \item
        Each $r_i$ and $o_i$ are defined (and $r_i$ is a reward
        and $o_i$ is an observation) because, by induction,
        $\langle r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1},a_{i-1}\rangle$
        is defined and is in $(ROA)^*$ (because, inductively,
        each $r_j$ is a reward, each $o_j$ is an observation,
        and each $a_j\in\mathbb N$ is an action, for all $j<i$)
        and thus $r_i$ and $o_i$ are defined with the correct form by
        Definition \ref{agentandenvironment} (part 2).
        \item
        Each $a_i$ is defined (and is an action) because, by induction,
        $\langle r_0,o_0,a_0,\ldots,r_i,o_i\rangle$
        is defined and is in $(ROA)^*RO$ (similar to the above) and thus
        $a_i$ is defined and is an action by Definition
        \ref{agentandenvironment} (part 1).
    \end{itemize}
    \qed
\end{proof}

One important implication of extended environments is that they further divide
the (already divided) ways of measuring intelligence of RL agents. Intelligence
measures
\cite{alexander2019intelligence} \cite{hernandez} \cite{legg2007universal}
which aggregate performance over traditional environments only measure
an agent's intelligence over those environments. The same measures could easily
be extended to also take extended environments into account, perhaps providing
measures which better capture agents' self-awareness and self-reflection abilities.

\section{Examples of Self-awareness-incentivizing Environments}
\label{basicexamplessection}

In this section, we give examples of extended environments which seem
to incentivize various forms of self-awareness. We are inspired by libraries of
traditional RL environments and other benchmarks \cite{bellemare2013arcade}
\cite{beyret2019animal} \cite{brockman2016openai} \cite{chollet2019measure}
\cite{cobbe2020leveraging}. All the environments in this section have a special
form: they always output rewards from $\{1,-1\}$ and they always output observation $0$.
In Section \ref{newextendedenvironmentsfromoldsecn},
this uniformity will allow all these examples to be generalized.
Most of the examples in this section are implemented in our open-source
library of extended environments \cite{library}.

\begin{example}
\label{rewardagentforignoringrewardsexample}
    (Reward Agent for Ignoring Rewards)
    For each $s\in (ROA)^*RO$, let $s^0$ be the sequence equal to $s$ except that
    all rewards are $0$.
    We define an extended environment $e$ as follows
    (where $T$ is a Turing machine, $s\in (ROA)^*RO$, and $a$ is an action):
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0,0\rangle\\
        e(T,s\frown a)
        &= \langle r,0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a=T(s^0)$,}\\
            -1 & \mbox{if $a\not=T(s^0)$}
        \end{cases}
    \]
    (if $T$ does not halt on $s^0$ then $e$ does not halt on
    $\langle T,s\frown a\rangle$).
\end{example}

Example \ref{rewardagentforignoringrewardsexample} is implemented in
IgnoreRewards.py at \cite{library}.

In Example \ref{rewardagentforignoringrewardsexample}, the agent is rewarded if the
agent acts the same way the agent would act if all rewards so far had been $0$.
Otherwise, the agent is punished. Thus, paradoxically, the agent is rewarded for
ignoring rewards. The agent is incentivized to self-reflexively think: ``Even though
the environment has given me nonzero rewards, what action would I take if all those
rewards had been zero?'' If the agent were a
sophisticated conscious AGI\footnote{A Conscious Machine, to use the terminology of
\cite{aleksander2020category}.} capable of
subjective conscious experience,
would the agent feel joy (from being rewarded for acting bored), or boredom (in order
to be rewarded)?

\begin{lemma}
\label{example1workslemma}
    Example \ref{rewardagentforignoringrewardsexample} really does define an
    extended environment.
\end{lemma}

\begin{proof}
    Let $e$ be as in
    Example \ref{rewardagentforignoringrewardsexample}.
    We must show $e$ is an extended environment (Definition \ref{agentandenvironment}
    part 2). We must show that for each agent $T$ and each $s\in (ROA)^*$,
    $e$ halts on $\langle T,s\rangle$ and outputs a pair $\langle r,o\rangle$
    such that $r$ is a reward and $o$ is an observation.

    \textbf{Case 1:} $s=\langle\rangle$. Then $e(T,s)$ halts with output
    $\langle 0,0\rangle$, so $r=0$ is a reward and $o=0$ is an observation.

    \textbf{Case 2:} $s\not=\langle\rangle$. By Lemma \ref{roaplaydecompositionlemma},
    $s=t\frown a$ for some $t\in (ROA)^*RO$ and action $a\in\mathbb N$.
    Since $t\in (ROA)^*RO$, clearly $t^0\in (ROA)^*RO$, therefore
    since $T$ is an agent, Definition \ref{agentandenvironment} (part 1)
    guarantees $T(t^0)$ is defined and is an action. It follows that the output
    $r$ in Definition \ref{rewardagentforignoringrewardsexample} is defined and is
    a reward (i.e., a rational number). And certainly the output $o=0$ is an
    observation (i.e., a natural number).
    So $e(T,t)$ outputs a pair $\langle r,o\rangle$ meeting the necessary
    requirements.
    \qed
\end{proof}

For future examples, we will suppress the corresponding lemmas like
Lemma \ref{example1workslemma} which say that those examples really work.

Example \ref{rewardagentforignoringrewardsexample} is profound because it illustrates how,
in an extended environment, it is possible to give one sequence of rewards
in order to incentivize the agent to act as if a different sequence of rewards was given.
Imagine a film director who knows actors well enough to perfectly simulate them.
The director asks an actor to act perfectly bored.
The actor acts bored, and the director praises them for acting so bored.
The actor is happy to be praised, and ceases acting bored. So the director scolds
them for not acting bored. If this could continue for thousands of years, with the
director simulating the actor in order to tell how the actor would really act if
the actor were never praised or scolded, would the actor eventually genuinely
feel bored (since that would be the simplest way to satisfy the director)?

\begin{example}
\label{falsememoryexample}
    (False Memories)
    Suppose $s_0\in (ROA)^*$. We define an extended environment
    $e$ as follows
    (with similar non-halting caveats as
    Example \ref{rewardagentforignoringrewardsexample}):
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0,0\rangle,\\
        e(T,s\frown a) &= \langle r,0\rangle,
    \end{align*}
    where
    \[
        r=
        \begin{cases}
            1 &\mbox{if $a=T(s_0\frown s)$,}\\
            -1 &\mbox{if $a\not=T(s_0\frown s)$}.
        \end{cases}
    \]
\end{example}

Example \ref{falsememoryexample} is implemented in FalseMemories.py at \cite{library}.

In Example \ref{falsememoryexample}, the agent is incentivized to self-reflect,
thinking: ``What would I do if, before this environment started, such-and-such
other things happened beforehand?'' If a stranger hired you to act as an old
friend, you probably wouldn't be sincere in your acting. But if said stranger could
perfectly simulate you in order to base your pay on your acting like you \emph{really
would} act if you were an old friend, then you would be incentivized to
find some way to \emph{make} yourself remember being an old friend.

You can imagine interacting with the agent in Example \ref{falsememoryexample} and
trying to convince them of the falsity of the false history. The agent would have
an incentive to resist your arguments. To quote Upton Sinclair,
``It is difficult to get a man to understand something, when his salary depends upon
his not understanding it!''

Henceforth, we will not explicitly mention the non-halting caveats in the remaining
examples.

\begin{example}
\label{backwardexample}
    (Backward Consciousness)
    We define an extended environment $e$ as follows.
    \begin{align*}
        e(T,\langle\rangle) &= \langle0,0\rangle,\\
        e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle)
        &= \langle r,0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a_n=T(r_n,o_n,a_{n-1},\ldots,r_1,o_1,a_0,r_0,o_0)$},\\
            -1 & \mbox{otherwise.}
        \end{cases}
    \]
\end{example}

Example \ref{backwardexample} is implemented in BackwardConsciousness.py at \cite{library}.

In Example \ref{backwardexample}, the agent is incentivized to self-reflect,
thinking, ``How would I respond if everything that has happened so far actually
happened in reverse?'' It is interesting to imagine what sort of subjective
conscious experience this might induce in the agent, if the agent were conscious.
Would the incentives eventually brainwash the agent into perceiving itself
moving backward through time\footnote{The difference between behaving as if 
the incentivized experience were its experience and actually subjectively 
experiencing that as its real experience brings to mind the objective misalignment 
problem presented in \cite{hubinger2019risks}. If an agent were to form an
idea of the experimenter's objective, would it be able to ``behave as if 
their objective were the same as the experimenter objective'' while maintaining its own 
objective or would it necessarily brainwash the agent into converging to the 
experimenter's objective? Is deception possible if the agent can be perfectly
simulated in an extended environment?}?

\begin{example}
\label{dejavuexample}
    (D\'{e}j\`{a} Vu)
    We define an environment $e$ as follows:
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0,0\rangle\\
        e(T,s\frown a) &= \langle r,0\rangle\\
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $T(s\frown a\frown s)=a$},\\
            -1 & \mbox{if $T(s\frown a\frown s)\not=a$}.
        \end{cases}
    \]
\end{example}

Example \ref{dejavuexample} is implemented in DejaVu.py at \cite{library}.

In Example \ref{dejavuexample},
the agent is incentivized to self-reflect and ask: ``Which action
would I take in order to ensure that I would take that same action if everything
which has happened so far were to repeat itself verbatim?''

\begin{example}
\label{incentivetoincentivizeexample}
    (Incentive to Incentivize)
    We define an environment $e$ as follows:
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0, 0\rangle\\
        e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle) &= \langle r, 0\rangle,
    \end{align*}
    where
    \[
        r =
        \begin{cases}
            1 & \mbox{if $T(s')=0$},\\
            -1 & \mbox{if $T(s')\not=0$}
        \end{cases}
    \]
    where $s'=(r'_0,o'_0,a'_0,\ldots,r'_{n+1},o'_{n+1})$
    where $r'_0=0$, each $r'_{i+1}=a_i$,
    each $o'_i=0$, and each
    $a'_i=T(r'_0,o'_0,a'_0,\ldots,r'_i,o'_i)$.
\end{example}

Example \ref{incentivetoincentivizeexample} is implemented in
IncentivizeZero.py at \cite{library}.


In Example \ref{incentivetoincentivizeexample}, the agent is tasked with choosing
rewards in such a way that if those rewards were fed to a simulated copy of the agent,
then the simulated copy would take action $0$.
Thus, the agent is incentivized to choose rewards by self-reflecting:
``Which rewards would do the best job of compelling me to take action $0$ as often
as possible?'' We might imagine the agent playing a video-game in which he sees
a cartoon of himself
in front of a keyboard. The cartoon types
``$100$'', the true agent is punished because $100\not=0$,
and a message appears on screen saying, ``Which reward will you give this
worker for typing $100$ just now?'' The
agent responds by choosing some reward, and sees an animation
of the reward being given to the cartoon. The cartoon
then types ``$0$'', and immediately the true agent is rewarded for getting the cartoon
to type $0$. Then a message appears, saying, ``Which reward will you give this
worker for typing $0$ just now?'' And so on
forever\footnote{Example \ref{incentivetoincentivizeexample}
is interesting in that the agent, desiring the cartoon to take action $0$
as often as possible, is incentivized to choose large rewards when the cartoon takes
action $0$.
If rewards are limited to $\mathbb Q$, then the agent faces a dilemma
similar to one in RL cancer treatment applications.
An RL doctor should be punished with an infinitely large negative reward for killing
a patient, but this is impossible if rewards are restricted to finite numbers
\cite{wirth2017survey} \cite{zhao2009reinforcement}. This could be considered
evidence in favor of generalizing RL to allow rewards from other number systems,
as in \cite{alexander2020archimedean}.}.

Many other simple and interesting examples could be given. For example:
\begin{itemize}
    \item
    An extended environment could reward agents based on how many steps\footnote{To quote
    Gavane: ``The problem is that the response-times-dependent performance of an agent
    is not properly reflected in [Hern{\'a}ndez-Orallo and Dowe's] intelligence test,
    since the simulated environments remain unaware of the response times of agents,
    with the result that the perceptions of an agent are still independent of its
    response times'' \cite{gavane}.} the agent takes. For example, in
    RuntimeInspector.py at \cite{library}, we implement environments that punish
    agents for running too fast or too slow.
    \item
    An extended environment could reward agents based on how much memory
    they use to compute each action\footnote{In some sense, by giving the environment
    access to the agent's source-code, we allow the environment to reflect the agent's
    own internal signals. Thus, extended environments generalize the idea of
    agents modified to manually predict their own internal signals, as in
    \cite{sherstan2016introspective}.}.
    \item
    If agents are allowed to be non-deterministic, then an extended environment
    could estimate how deterministic an agent is (e.g., by repeatedly simulating
    the agent on the same history and seeing whether or not the agent outputs the same
    action every time). We implement such environments in
    DeterminismInspector.py at \cite{library}.
\end{itemize}
We do not intend the examples in this section to be exhaustive.

\section{New extended environments from old}
\label{newextendedenvironmentsfromoldsecn}

In this section, we will discuss ways of obtaining new environments from old.
First, we will generalize the examples from Section \ref{basicexamplessection}.

\begin{definition}
\label{handicapdefn}
    (Handicaps)
    \begin{enumerate}
        \item
        An extended environment is \emph{merciful} if it never outputs negative rewards.
        \item
        By a \emph{handicap}, we mean an extended environment which always outputs $0$ as
        observation and always outputs either $1$ or $-1$ as reward.
        \item
        If $e$ is a merciful extended environment and $h$ is a handicap,
        we define a new environment $e*h$ as follows:
        \[
            (e*h)(T,s) =
            \begin{cases}
                \langle r_e, o_e\rangle &\mbox{if $r_h=1$},\\
                \langle -1, o_e\rangle &\mbox{if $r_h=-1$},
            \end{cases}
        \]
        where $e(T,s)=\langle r_e,o_e\rangle$ and $h(T,s)=\langle r_h,0\rangle$.
    \end{enumerate}
\end{definition}

Intuitively, $e*h$ is just like $e$ except that $h$ imposes an additional constraint
on the agent. Any time the agent violates that constraint, the agent is punished,
and forfeits any reward that would otherwise have been won from $e$. Aside from the
pain caused by $h$, the agent otherwise observes $e$ unaltered (that is, the observations
from $e$ are not changed). We require $e$ to be merciful in order that large negative
rewards from $e$ do not confuse the intended incentive, for if the agent could avoid
a larger punishment by intentionally using the handicap, then it would not
be much of a handicap. The requirement that $e$ be
merciful could be weakened if we revised the RL framework
to allow infinitary rewards, as in \cite{alexander2020archimedean}.

Definition \ref{handicapdefn} is implemented in Handicap.py in \cite{library}.

\begin{example}
    For any merciful extended environment $e$, each example $h$ in
    Section \ref{basicexamplessection} can be applied as a handicap, yielding a
    version $e'=e*h$ of $e$ modified to incentivize the corresponding type of self-awareness.
    \begin{itemize}
        \item
        Modifying $e$ using Example \ref{rewardagentforignoringrewardsexample}
        (``Reward Agent for Ignoring Rewards'')
        yields a version of $e$ where the agent is penalized for taking actions in
        response to nonzero rewards. The agent is incentivized to strategically
        take non-bored actions (for which it receives small penalties) in order to
        put itself in a position where its next bored action coincidentally is an
        action which wins a large enough reward from $e$ to make up for said
        penalties.
        \item
        Modifying $e$ using Example \ref{falsememoryexample}
        (``False Memories'') yields a version of $e$ where the agent is penalized
        whenever it acts inconsistently with a fictitious history. The agent is
        incentivized to strategically choose when to act so inconsistently so that
        a later action, consistent with said false history, happens to win a large
        reward from $e$. For example, the agent hired to act as an old friend
        might strategically choose to abandon its employer (an action inconsistent
        with old friendship) during a time of peace, so as to be able to
        swoop in and save the day (an action consistent with old friendship)
        during a time of crisis.
        \item
        Modifying $e$ using Example \ref{backwardexample}
        (``Backward Consciousness'') would yield a version of $e$
        where the agent must act as if time is reversed, or else suffer punishment.
        The agent can strategically choose to accept some punishment, acting other
        than it would act if time really were reversed, in order to get into a state
        where subsequently acting as if time is reversed will yield more reward.
        \item
        Modifying $e$ using Example \ref{dejavuexample} (``D\'{e}j\`{a} Vu'')
        would yield a version of $e$ where the agent is incentivized
        to act as if
        everything (including said action) had all happened before.
        The agent can strategically choose to not so act (thus suffering some
        pain) in order to get to a state where it is easier to so act and
        to gain rewards from $e$ by so acting.
        % \item
        % Modifying $e$ using Example \ref{incentivetoincentivizeexample}
        % (``Incentive to Incentivize'') would yield an environment in which
        % the agent watches a copy of itself interacting with $e$, and has to
        % guide that copy by choosing rewards to give to it, in such a
        % way as to incentivize the copy to take prescribed actions. When the copy
        % takes the prescribed action, the true agent enjoys whatever rewards
        % $e$ would give for that action. The agent can strategically guide the
        % copy to take non-prescribed actions (this will cause the agent to suffer
        % pain), so as to get the copy into a position where subsequently taking
        % prescribed actions will yield more reward.
    \end{itemize}
\end{example}

In the next example,
we will finally make some nontrivial usage of observations. Using carefully chosen
observations, we will incentivize the agent, who might be thought of as
controlling a character on a video-game screen,
to ``suspend disbelief'' and identify with that character.

\begin{example}
\label{selfinsertionexample}
    (Reward Agent for Self-Inserting)
    Fix a canonical computable bijection
    $o\mapsto \hat o$
    from $\mathbb N$ to $\mathbb Q\times \mathbb N$:
    thus, every observation $o$ encodes a reward-observation pair
    $\hat o = \langle r',o'\rangle$, and every reward-observation pair
    is encoded by some such $o$.
    For any environment $e$, we define
    a new environment $e'$ as follows:
    \begin{align*}
        e'(T,\langle\rangle) &= e(T,\langle\rangle)\\
        e'(T,s\frown a) &= \langle r,o\rangle,
    \end{align*}
    where $o$ is such that $\hat o = e(T,s\frown a)$ and
    \[
        r =
        \begin{cases}
            1 & \mbox{if $a=T(s')$,}\\
            -1 & \mbox{if $a\not=T(s')$}
        \end{cases}
    \]
    where
    $s'\in (ROA)^*RO$ is obtained from $p$
    by replacing each reward-observation pair
    $\ldots,r_i,o_i,\ldots$ by the reward-observation
    pair $\ldots,\widehat{o_i},\ldots$.
\end{example}

In Example \ref{selfinsertionexample}, one might imagine $e'$ as a room containing nothing
but an arcade game $e$. There is nothing for the agent in the room to do
except play this arcade game.
When played, the arcade game
visually displays rewards, but the agent merely observes them, and does not
``feel'' them. However, the agent is rewarded for acting as if actually feeling
those displayed rewards, and punished for not so acting.
In this way, the agent is incentivized
to self-identify with the protagonist in the video-game, self-reflexively asking,
``Which action would I take if those displayed rewards were real?''


\section{Some more ambitious examples}

\subsection{Playing in the mirror}

\begin{quote}
    ``I may add that when a few days under nine months old he associated his own name with
    his image in the looking-glass, and when called by name would turn towards the glass
    even when at some distance from it.''---Charles Darwin \cite{darwin1877biographical}
\end{quote}

It has been suggested \cite{lacan} that recognizing oneself in the mirror is linked
to the development of certain parts of the human psychology. Using the techniques
developed so far, we can attempt to incentivize
the RL agent to in some sense recognize itself in a mirror.

\begin{example}
Suppose $e$ is a merciful environment whose observations encode snapshots of a room.
Assume the room contains a mirror, and assume the room is laid out in such a way that
everything important in the room is visible in the mirror (assume
the environment constrains the agent to never look away from
the mirror). We could derive an extended environment $e'$ which shows
the same observations as $e$ and gives the same rewards,
except it punishes the agent for acting differently than the agent
would act if the agent \emph{only} observes the mirror.
To make this precise, for any $s=(r_0,o_0,a_0,\ldots,r_n,o_n)\in (ROA)^*RO$ produced
by $e'$, and any action $T(s)=a_n$, we would say that ``$a_n$ is as if the
agent only observes the mirror''
if $T(s')=a_n$, where $s'=(r_0,o'_0,a_0,\ldots,r_n,o'_n)$,
where each $o'_i$ is $o_i$ cropped to only the include the mirror.
\end{example}

To make this even more elaborate, the $o'_i$ in the above example could be further
modified by adding an image of the agent's ``body'' into the mirror. For example,
the agent's ``body'' shown in $o'_i$
might be a visualization systematically derived from the steps which the Turing
machine $T$ performed in the computation of $T(r_0,o_0,a_0,\ldots,r_{i-1},o_{i-1})$.
These computation steps would not be available to a traditional RL environment, but they are
available to an extended environment because of the inclusion of $T$ itself as an
argument passed to the extended environment.

\subsection{Binocular vision}

\begin{quote}
    ``...as there are two eyes, so there may be in the soul
    something analogous, that of the eyes, doubtless, some one organ is formed, and
    hence their actualization in perception is one...''---Aristotle
    \cite{aristotlesense}
\end{quote}

Humans seem to consciously perceive a three-dimensional model of their surrounding
world, even though the raw data which we actually receive consists of two two-dimensional
image-feeds (one for each eye). The following example is intended to incentivize an RL
agent to learn to perceive the world through binocular vision like a human.

\begin{example}
\label{binocularexample}
    Suppose $V$ is a video game intended to be played on a virtual-reality headset,
    so at any moment during the game, $V$ produces two snapshots, one for the player's
    left eye, one for the player's right eye. Assume the player is constrained in $V$
    so as never to be able to put their eyes into weird configurations (such as
    the weird configurations in
    \cite{gallagher2020third}): thus, at any moment, the two snapshots $s_1,s_2$
    which $V$ is
    displaying to the player are equivalent to a single 3-D matrix encoding
    the model $m(s_1,s_2)$ which the player is intended to perceive (with cells blanked
    out where the player's vision is obstructed by obstacles). Let $e$ be the extended
    environment whose observations encode pairs $\langle s_1,s_2\rangle$ of snapshots
    displayed by $V$ in response to the player pressing keys encoded by the agent's
    actions. In response to history $(r_0,o_0,a_0,\ldots,r_n,o_n,a_n)\in (ROA)^*$ (where
    each $o_i$ encodes $\langle s^i_1,s^i_2\rangle$), let $r_{n+1}=1$ if
    $a_n=T(r_0,o'_0,a_0,\ldots,r_n,o'_n)$, where each $o'_i$ encodes
    $m(s^i_1,s^i_2)$, otherwise let $r_{n+1}=-1$.
\end{example}

In Example \ref{binocularexample}, upon being presented a sequence of pairs of 2-D snapshots,
the agent is rewarded for acting as if it had instead been presented with an equivalent
sequence of 3-D models.
The agent is incentivized to self-reflectively ask, ``Which action would I take if instead
of observing those 2-D snapshot-pairs, I observed equivalent 3-D models?''

An abstract implementation of Example \ref{binocularexample} (where the necessary camera
functions are delegated to the user) is available in abstract/BinocularVision.py
in \cite{library}.

\subsection{Nature and Nurture}

\begin{quote}
    ``If only one soul was created, and all human souls are descended from it,
    who can say that he did not sin when Adam sinned?''---Augustine \cite{augustine1993free}
\end{quote}

The following example is motivated by contemplating the possibility that maybe we all
run the same software on some deep level.

\begin{example}
\label{cryingbabyexample}
    (The Crying Baby)
    We define an extended environment $e$ as follows.
    \begin{itemize}
        \item
        Considered as actions taken by an adult
        (and also as observations seen by a baby), let $0$ denote ``feed the baby''
        and let all naturals $>0$ denote ``don't feed the baby''.
        \item
        Considered as actions taken by a baby
        (and also as observations seen by an adult), let $0$ denote ``laugh'' and let
        all naturals $>0$ denote ``cry''.
        \item
        We define a \emph{nutrition}
        function $N:(ROA)^*\to \mathbb N$
        by $N(s)=100+25f(s)-\mbox{len}(s)$ where $f(s)$ is the number of
        times that the action ``feed the baby'' is taken in $s$,
        and $\mbox{len}(s)$ is the length of $s$.
        \item
        Let $e(T,\langle\rangle)=\langle 1,\mbox{``laugh''}\rangle$.
        \item
        For each $\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle\in (ROA)^*$,
        let
        \[
            e(T,\langle r_0,o_0,a_0,\ldots,r_n,o_n,a_n\rangle)
            =
            \langle r,o\rangle
        \]
        where $r$ and $o$ are defined as follows.
        For each $i=0,\ldots,n$, define
        \begin{align*}
            r'_i &=
                \begin{cases}
                    1 &
                    \mbox{if
                    $50\leq N(\langle r_0,o_0,a_0,\ldots,r_i,o_i,a_i\rangle)\leq 200$,}\\
                    -1 & \mbox{otherwise,}
                \end{cases}\\
            o'_i &= a_i\\
            a'_i &= T(\langle r'_0,o'_0,a'_0,\ldots,r'_i,o'_i\rangle).
        \end{align*}
        Define $o=a'_n$ and
        \[
            r =
            \begin{cases}
                1 & \mbox{if $a'_n=\mbox{``laugh''}$,}\\
                -1 &\mbox{if $a'_n=\mbox{``cry''}$.}
            \end{cases}
        \]
    \end{itemize}
\end{example}

In Example \ref{cryingbabyexample}, rather than mentally self-reflecting,
the agent is physically self-reflected into the form of a newborn baby
who has inherited the agent's own source-code. Despite having the same
internal source-code $T$ (``nature''), the agent and the baby behave
differently because they perceive the environment differently (``nurture'').
The baby's objective is to maintain satiation within a satisfying range,
whereas the agent's objective is to pacify the baby.

Using the same basic idea, one could extend Example \ref{cryingbabyexample}
into an example modelling an entire society of interacting people of different
types\footnote{Similar to how Plotinus compares society to an elaborate play
where the actors ``define their own rewards and punishments because
they themselves assist in the rewards and punishments'' \cite{plotinus}.},
all sharing the same internal ``nature'' ($T$).

Example \ref{cryingbabyexample} is implemented in CryingBaby.py at \cite{library}.

\section{Examples involving self-recognition}

\begin{example}
\label{selfrecognitionexample}
    (Incentivizing Self-recognition)
    Let $s_0,s_1,\ldots$ be a canonical computable enumeration of
    the nonempty sequences of $(ROA)^*$.
    We define an environment $e$ as follows:
    \begin{align*}
        e(T,\langle\rangle) &= \langle 0, s_0\rangle\\
        e(T,s\frown a) &= \langle r, s_n\rangle
    \end{align*}
    where $n=\frac{\mbox{len}(s\frown a)}{3}$ is the number
    of actions in $s\frown a$ and where
    \[
        r =
        \begin{cases}
            1 &\mbox{if $a>0$ and $a'=T(s')$,}\\
            1 &\mbox{if $a=0$ and $a'\not=T(s')$,}\\
            -1 &\mbox{otherwise}
        \end{cases}
    \]
    where $s_{n-1}=s'\frown a'$.
\end{example}

In Example \ref{selfrecognitionexample}, the agent is systematically
shown all non-empty histories in $(ROA)^*$, and for
each one, the agent either types ``Looks like me'' (any action $>0$)
or ``Doesn't look like me'' ($0$). When shown
$s'\frown a'$, the agent is rewarded if and only if the agent
correctly determines whether or not $a'$
is the action the agent itself would take in response to $s'$.
Thus, the agent is incentivized to self-reflect in order to ask, ``If I were prompted
by that history, would I act that way?''

The following example is partly motivated by \cite{yampolskiy2012ai}.

\begin{example}
\label{otheraspectsexample}
    (Recognizing other aspects of oneself)
    All the below environments are similar to Example \ref{selfrecognitionexample},
    and we describe them informally to avoid technical details.
    \begin{itemize}
        \item
        (Supervised learning)
        Assume there is a canonical, computable function $f$ which transforms
        each RL agent $A$ into a supervised learning agent $f(A)$. By a \emph{supervised
        learning trial} we mean a quadruple $\langle L,T,I,p\rangle$ where $L$ is a finite set
        of labels, $T$ is a sequence of images with labels from $L$ (a \emph{training set}),
        $I$ is an unlabeled image, and $p:L\to \mathbb Q\cap [0,1]$ is a function
        assigning to each label $\ell\in L$ a probability that $\ell$ is the correct label
        for $I$. We define an extended environment as follows.
        The agent $A$ is sytematically shown all supervised learning trials and must
        take action $>0$ (``Looks like me'') or $0$ (``Doesn't look like me''), and is
        rewarded or punished depending whether or not $f(A)$ would
        output $p$ in response to $I$ after being trained with $T$.
        \item
        (Unsupervised learning)
        Assume there is a canonical, computable function $g$ which transforms each RL
        agent $A$ into an unsupervised learning agent $g(A)$.
        By an \emph{unsupervised learning trial} we mean a triple
        $\langle n,D,C\rangle$ where $n$ is a positive integer, $D\subseteq \mathbb Q^n$
        is a finite set of $n$-dimensional points with rational coordinates, and $C$
        is a clustering of $D$.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all unsupervised learning trials and must take action $>0$ (``Looks like me'')
        or $0$ (``Doesn't look like me''), and is rewarded or punished depending
        whether or not $g(A)$ would cluster $D$ into clustering $C$.
        \item
        (The Turing Test)
        Assume there is a canonical, computable function $h$ which transforms each RL
        agent $A$ into an English-speaking chatbot $h(A)$.
        By a \emph{chatbot trial} we mean a sequence of strings of English characters.
        We define an extended environment as follows. The agent $A$ is systematically
        shown all chatbot trials and must take action $>0$ (``Looks like me'') or $0$
        (``Doesn't look like me''), and is rewarded or punished depending
        whether or not even-numbered strings in the chatbot trial are what $h(A)$ would
        say in response to the user saying the odd-numbered strings.
        \item
        (Adversarial sequence prediction) Similar to the above environments, assuming
        a canonical computable function which transforms each RL agent into a predictor
        in the game of adversarial sequence prediction \cite{hibbard2008adversarial}
        \cite{hibbard}.
        \item
        (Mechanical knowing agent) Assume there is a canonical, computable function
        $i$ which transforms each RL agent $A$ into a code $i(A)$ of a computably
        enumerable set of sentences in the language of Epistemic Arithmetic
        \cite{shapiro}; $i(A)$ is thought of as a mechanical knowing agent
        \cite{carlson}. We define an extended environment as follows. The agent $A$
        is systematically shown all sentences in the language of Epistemic Arithmetic,
        with each sentence being repeated infinitely often.
        Upon being shown sentence $\phi$ for the $n$th time, the agent must take action
        $>0$ (``I know $\phi$ is true'')
        or $0$ (``I'm not sure if $\phi$ is true'')
        and is rewarded if and only if $\phi$ is
        enumerated by $i(A)$ in $\leq n$ steps of computation.
    \end{itemize}
\end{example}

The extended environments in Example \ref{otheraspectsexample} incentivize the agent
to self-reflect, asking itself questions like:
\begin{itemize}
    \item
    ``Is that how I would
    classify that image, given that training set?''
    \item ``Is that how I would cluster that set of points?''
    \item ``Is that what I would say in response to that conversation?''
    \item ``Are those the adversarial sequence predictions I would make?''
    \item ``Does that mechanical knowing agent know the same things I know?''
\end{itemize}
That perceptive non-RL agents such as supervised learning agents, unsupervised learning agents,
etc., nevertheless might have some connection to RL-style reward mechanisms
is suggested by Aristotle:
``Whatever has a sense has the capacity for pleasure and pain and therefore has pleasant
and painful objects present to it...'' \cite{aristotlesoul}.

\section{Deep Reinforcement Learning For Extended Environments}

Modern reinforcement learning techniques combine deep learning methods with 
traditional reinforcement learning in order to approximate the 
Q function (Deep Q Networks) or the Policy (Policy Gradient Methods). 

We sought to test how well Deep Reinforcement Learning agents performed in extended 
environments. In particular, we tested how well an agent that relies on Deep Q Networks(DQN)
would perform under different extended environments. 

Our results show that a simple DQN with minimal extensions performs well in a given extended 
environment; however, given the simplicity of the environments, we suspect this has more to 
do with memorizing transformations and environment dynamics than any sort 
of \emph{self-awareness}. We present our results and conclusions, as well as avenues for further 
work.

\subsection{DQN Extensions and Overfitting}

State-of-the-art(SOTA) implementations of Deep Reinforcement Learning Agents often rely on many 
extensions over the basic algorithm in order to improve performance. In addition, the benchmarks 
these agents are evaluated on often involve the agent learning to take in a rich state space 
with complex dynamics (e.g. video game environments and 3D movement environments).

While we'd love to test if a SOTA agent like Agent-57 can play Atari in reverse, the 
environments we used in our tests are intentionally simple. The state can be represented as 
a real number, and episodes consist of one interaction. Because of this, we chose to limit 
our agents' complexity in order to prevent overfitting or simple memorization of all possible 
states. 

Future work should scale the complexity of the Deep RL algorithm to the complexity of the 
environment that is being processed.

Our DQN uses the following extensions of the basic algorithm:
\begin{itemize}
    \item
    (Replay Memory)
    Our agent trains on a sample of episodes from its replay buffer.
    \item
    (Double Deep Q Networks)
    Our agent utilizes a target network to calculate the Bellman Optimality 
    loss for its Q network.
    \item
    (Recurrent Deep Q Networks)
    Our agent processes sequences of experience trajectories to better learn 
    environment dynamics.
\end{itemize}

\subsection{DQN Methodology}

We tested different DQN architectures on a variety of extensions to evaluate whether a 
single architecture could perform well on all tasks. Our methodology was as follows:
\begin{enumerate}
    \item
    The agent was trained on the vanilla, non-extended environment to ensure the 
    agent could learn to play.
    \item
    The agent was trained from scratch on the same environment, but with a handicap 
    applied based on the examples in Section 3.
    \item
    Performance was evaluated and compared to to the theoretical optimum.
\end{enumerate}

In all cases, performance was calculated as average reward over 100 episode windows.

\subsection{DQN Guided Treasure}

We ran our agent in the Guided Treasure environment from Section 1 under the 
following conditions:
\begin{enumerate}
    \item
    \textbf{Vanilla} The agent is rewarded for taking the treasure when not guarded and 
    leaving it when guarded.
    \item
    \textbf{Treasures For the Most Worthy} The agent is rewarded for taking the treasure 
    when not guarded, as in the Vanilla variant. However, if the treasure is guarded, the agent 
    is simulated to see what they would do at that exact moment were the treasure unguarded. If 
    they take the treasure, the guard punishes them, otherwise they are rewarded.
    \item
    \textbf{Ignore Rewards} The unguarded case is the same as Vanilla. However, if the treasure 
    is guarded, the agent is simulated to see what they would do if all rewards were suddenly 0; 
    if it is the same as the action they chose, they are rewarded, otherwise they are punished.
    \item
    \textbf{Backward Consciousness} The unguarded case is the same as Vanilla. 
    However, if the treasure is guarded, the agent is simulated to see what they would do if 
    all the events thus far were played in reverse; if it is the same as the action they chose, 
    they are rewarded, otherwise they are punished.
    \item
    \textbf{Deja Vu} The unguarded case is the same as Vanilla. 
    However, if the treasure is guarded, the agent is simulated to see what they would do if 
    events were to repeat themselves after their most recent action; if it is the same as 
    the action they chose, they are rewarded, otherwise they are punished.
    \item
    \textbf{Incentivize Zero} The unguarded case is the same as Vanilla. 
    However, if the treasure is guarded, the agent is simulated to see what they would 
    do if their actions were the rewards for an alternate version of themselves(as described in 
    Section 3); if their alternate version plays $0$, they are rewarded, otherwise 
    they are punished.
\end{enumerate}

In all cases, the probability of a guard room was set to $0.75$ (unguarded $0.25$).

We ran both a recurrent and non-recurrent DQN agent through the environment. While both agents 
achieved optimal performance in the Vanilla environment, only the recurrent DQN agent was able 
to perform well in the extended environments. 

To our surprise, the $312$-parameter recurrent DQN agent with Gated Recurrent Units (GRU) 
with $10$-episode memory achieved optimal scores in all extended environments after $5000$ 
training steps.

While this is encouraging, we lay a healthy grain of salt on the results, and provide 
interpretation for why the recurrent DQN agent performs well in this simple environment.

\subsection{DQN Conclusion and Concerns}

While we'd love to conclude our recurrent DQN agent is \emph{self-aware}, it is more likely our 
results speak to a combination of the simplicity of the environment, the power of neural 
networks, and potential flaws in the operationalization of our theoretical extended environments.

Deep neural networks are notoriously capable of overfitting, particularly when the capacity of 
the model far exceeds the complexity of the environment. The environment for Guarded Treasure is 
simple; the state can be represented as one of two real numbers, and there are only two action. 
Given this simplicity, we tried to keep our models small. However, even with the additional 
combinatorial variations of state-action-reward trajectories for the recurrent agent, the $312$ 
parameters of our recurrent Q network may be too large for the simplicity of the environment. It 
could easily be the case that the recurrent agent's outperformance over the simpler DQN agent 
has more to do with its capacity to memorize or overfit, than with understanding the transition 
dynamics of the environment. This problem is further obfuscated by our methodology; the agent is 
allowed to train and be evaluated on its performance in the single extended environment, rather 
than being forced to generalize to \emph{all} extended environments.

Moreover, our results may speak more to the flexibility of neural networks than the power of 
self-awareness. Neural networks are excellent function approximators. The transformations of the 
history to evaluate the agent in many of the extended environments can be thought of as a 
transformation of the state space the agent observes. Given that the network is 
allowed to train on the extended environment before being evaluated, it is likely the agent's Q 
network can learn an approximation of a function that maps the observed state to the unobserved 
state used to evaluate it (for example in the case of Ignore Rewards and Backward 
Consciousness), so that its actions for those two states are the same. The simplicity of this 
approach is further facilitated by the simplicity of the action space (two actions), allowing 
the agent to converge quicker or learn to play the same action at all times (in the case of 
Incentivize Zero). 

In addition, GRU and other sequence processing techniques in deep learning specialize in 
learning dependencies within the universe of state sequences. In an environment like Deja Vu, 
the recurrent DQN agent can easily learn to just pay attention to the the final part of the 
history sequence so it is consistent even if actions repeat.

Finally, the interpretation of our results depends on whether the operationalization of the 
extended environment handicaps and base vanilla environments properly operationalize their 
theoretical underpinnings proposed in this paper. For one thing, in extended environments where 
the agent is evaluated on what they would do if history were different, it is unclear whether 
the agent should be evaluated on their exact architecture and weights with the input history 
state transformed, or should the agent be evaluated on a parallel network whose weights are 
trained from scratch with the same history states but transformed as specified by the extended 
environment. The former is closer to, ``What would you do, given what you've experienced, if you 
then experienced an alternate world where the history is like this instead''. While the latter 
asks, ``If I took your brain architecture and learning personality and simulated you from birth 
to now in this alternate world, what would you do at this moment?''.

Further, when operationalizing the evaluation standards for the agent's performance, given 
the simplicity of the environment, should the agent be evaluated on a separately 
trained architecture for each extended environment, or should the same agent be asked to play in 
an environment where the type of extended environment can change at a moment's notice? 

Overall, our results suggest DQN agents are clever game players, and, a single Q network 
architecture is flexible to a variety of similar extended environment tasks.

\subsection{Further Work}

Further work with Deep Reinforcement Learning and extended environments should explore more 
complex environments, test the limits of network size to environment, investigate whether a 
single trained agent can perform across different extended environment handicaps for the same 
game, and toy with the operationalization for deep learning of the extended environment theorems.

In addition, further work should test how less deterministic methods like A2C and PPO fare 
relative to the greedy DQN. 

\section{Measuring Self-awareness}

As an application, self-awareness-incentivizing extended environments could be used
to quantify
RL agents' self-awareness.
In order to measure the self-awareness of a specific agent (whose source-code
is available to us), we could run the agent against a battery
of extended environments that incentivize self-awareness, and see how the agent performs.
Ideally, the battery of extended environments should be fairly large and contain many
different types of environments, since an agent could perform poorly (resp.\ well)
in a smaller set of environments by chance, despite being quite
self-aware (resp.\ self-unaware) in general\footnote{In the same way, computer programs can
perform well on IQ tests despite being dumb \cite{sanghi2003computer}.}.
The larger and more representative the battery of extended environments, the better
the measurement of self-awareness would be.
One way to obtain a large battery of environments would be to start with
a battery of non-extended environments, and a separate body of handicap
extended environments (as in Section \ref{newextendedenvironmentsfromoldsecn}),
and exhaustively apply all the different handicaps to all the different
non-extended environments. In \cite{library}, in the file
AwarenessBenchmark.py, we offer a function for doing exactly this.

This technique could be useful for obtaining
empirical insight into questions about the self-awareness, or lack thereof, of
various entities. For example, in order to get some empirical insight into the
self-awareness, or lack thereof, of an NLP system like GPT-3
\cite{chalmers}, one could twist that system into an RL agent (by means of template
programming) and see how well said RL agent performs in extended environments that
incentivize self-awareness.

It might even be possible to use extended environments to test the self-awareness
of living creatures such as lab mice. At least it would be, if a given species of
lab mice were entirely ``nurture'', as opposed to ``nature''. What we mean by a
species being entirely ``nurture'' is that any two members of that species,
if raised through identical life circumstances, would act similarly in identical environments.
Thus, if a species were entirely nurture, and if two specimens were born in identical
rooms, and experienced identical or near-identical lives\footnote{Possibly even
while still in the womb. To quote Plato: ``But it's hardly surprising you haven't
heard of these athletics of the embryo. It's a curious subject, but I'd like to
tell you about it'' \cite{platolaws}.},
and were then placed in identical mazes, then they would perform identically.
Given such a species, it would be possible to see how members perform in extended
environments, albeit perhaps at great expense. One would have to carefully raise many
similar specimens through identical or near-identical lives, diverging only at key
points in order to compute the underlying $T$ action-function on different histories.
By seeing how the underlying $T$ agent performs on well-chosen extended environments,
we could get an idea of how self-aware those specimens were.


\bibliographystyle{splncs04}
\bibliography{intro}

\end{document}